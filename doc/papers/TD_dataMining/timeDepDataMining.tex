\section{Time Dependent Data Mining}
\label{sec:timeDepDataMining}

At this point we can make step further: making 
the algorithms shown in Section~\ref{} useful to deal with time dependent data. 

Using same analogy presented for static data we aim to group scenario with similar 
temporal patterns.
An example is shown in Fig.~\ref{fig:TD-clustering} applied to a data set containing the time evolution 
of 1000 time series has been generated by randomly changing (through a Monte-Carlo 
sampling) three variables (i.e., $x,y,z$). 
We introduced a discontinuity in the temporal evolution of the time series 
depending if $x>=4$ or $x<4$.

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.6]{TD-clustering.pdf}}
    \caption{Plot of a 1000 time series data set (right) and plot of the clustered data set colored 
             by the label assigned to each cluster.}
    \label{fig:TD-clustering}
\end{figure}

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.4]{hist2Clusters.pdf}}
    \caption{Histograms of the sampled values for $Cluster_0$ and $Cluster_1$ (shown in Figure 4) that 
             created them and were captured by the clustering algorithm.}
    \label{fig:hist2Clusters}
\end{figure}

By using A clustering algorithm we want to partition the 1000 generated scenario into 
2 clusters (see Fig.~\ref{}). Note how the scenarios in each cluster have a very similar 
temporal behavior. Then, by looking at the histograms of the sampled variables $x,y,z$ 
for the scenarios contained in each cluster we were able to verify that $x$ was creating 
the splitting of the data set. 

Figure~\ref{} shows the histograms of $x$ for both clusters: 
for Cluster\_0 $x<4$ while $x>4$ for Cluster\_1. Note that we would not have been able to capture 
this discontinuity by considering only the end or max values of the time series.

Again, we will indicate with $\Lambda$ the data set generate by any of the methods mentioned above 
which contain $N$ time series~\footnote{In this paper we will indicate time series as simulation 
runs or histories} 
$TS_n (n=1,\ldots,N)$:
\begin{equation}
  \Lambda = \{ TS_1,\ldots,TS_N \}  
  \label{eq:collectionTS}
\end{equation}
To preserve generality, given the complexity of the data generated by simulation based PRA 
methods such as RISMC, we now assume that each scenario $Η_n$ contains three components: 
\begin{equation}
  TS_n = \{ \bm{\Theta}_n,\bm{\Sigma}_n,\bm{\Gamma}_n \}  
  \label{eq:collectionTS}
\end{equation}

These components are the following:
\begin{itemize}
  \item Continuous data $\bm{\Theta}_n$: this data contains the temporal evolution of each scenario, i.e., 
        the time evolution
        of the $M$ state variables $x_m^n (m=1,\ldots,M)$ (e.g., pressure and temperature at a specific 
        computational node). 
        All of these state variables change in time $t$ (where $t$~\footnote{This allows us to 
        maintain generality by having time series with different time lengths} ranges from 0 to $t_n$): 
        \begin{equation}
          \bm{\Theta}_n = {x_1^n,\ldots,x_M^n}  
          \label{eq:bigTheta}
        \end{equation}  
        where each $x_m^n$ is a an array of values having length $T_n$. Hence, $\bm{\Theta}_n$ can be viewed 
        as a $M \times T_n$ matrix .
  \item Discrete $\bm{\Sigma}_n$: which contains timing of events. Note that a generic event $E_i^n$ can occur:
        \begin{itemize}
          \item At a time instant $\tau_i$: in this case the event can be defined as $(E_i^n,\tau_i)$, or,
          \item Over a time interval $[\tau_i^\alpha,\tau_i^\omega]$: in this case the event can be defined 
                as $(E_i^n,[\tau_i^\alpha,\tau_i^\omega])$
        \end{itemize} 
  \item Set $\bm{\Gamma}_n$ of $V$ boundary conditions $BC_v^n (v=1,\ldots,V)$ and $U$ initial 
        conditions $BC_u^n (u=1,\ldots,U)$.
\end{itemize}


\subsection{Approaches}
\label{sec:timeDepDataMiningApproaches} 
Two possible paths  that can be followed to analyze time dependent data:
\begin{itemize}
  \item Path 1 (see Fig.~\ref{fig:approach1}): Employ classical clustering algorithms by transforming each time series as 
                                  a single multi-dimensional vector. Recall that algorithms such as K-Means 
                                  and Mean-Shift can naturally deal with multi-dimensional vectors, i.e., each data 
                                  point can be represented as a multi-dimensional vector. Following this, in this path
                                  each time series is converted into a multi-dimensional vector (as part of a 
                                  pre-processing step). This can be done, for example, through a polynomial or Fourier 
                                  transformation (see Section~\ref{}).
  \item Path 2 (see Fig.~\ref{fig:approach2}): Maintain the original format of the time series end employ clustering algorithms that 
                                  can receive in input a distance matrix (thus appropriate distance metrics needs to be 
                                  defined). Few algorithms, such the hierarchical and the DBSCAN clustering
                                  algorithms can received in input the solely distance matrix $\Delta=[\delta_{ij}]$ where each 
                                  element $\delta_{ij}$ represent the distance between time series $i$ and $j$.
\end{itemize}

The advantage of the first path is that it employs a large variety of clustering algorithms that can handle very complex 
data sets (i.e., data points clustered in complex shapes other than ellipsoidal). On the other side, the 
conversion of the time series prior the clustering may cause erroneous results if clustering
parameters are not chosen properly (e.g., if the time series are very similar to each other and a coarse 
representation is chosen).

The second path, however, since it employs algorithms which can accent as input the distance matrix $\Delta$, they do not 
require any data transformation. When dealing with time series data the most important parameter to be considered 
here is the distance metric chosen to determine each element $\delta_{ij}$ of $\Delta$.

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.4]{approach1.pdf}}
    \caption{Analysis of time-dependent data using static representation conversion.}
    \label{fig:approach1}
\end{figure}

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.4]{approach2.pdf}}
    \caption{Analysis of time-dependent data using distance matrix based algorithms (e.g., Hierarchical).}
    \label{fig:approach2}
\end{figure}


\section{Data Pre-processing}
\label{sec:preProcessing} 

\subsection{Data Normalization}
\label{sec:dataNormalization} 

Depending on the application, the data set may need to be pre-processed. A common pre-processing method is the 
Z-normalization procedure: each variable $x_m^n$ of $\bm{\Theta}_n$ is transformed into $\tilde{x}_m^n$:

\begin{equation}
  \tilde{x}_m^n = \dfrac{(x_m^n-mean[x_m^n])}{stdDev[x_m^n]}  
  \label{eq:Znormalization}
\end{equation}

where $mean[x_m^n]$ and $stdDev[x_m^n]$ represent the mean and the standard deviation of $x_m^n$. 
This transformation provides an equal importance to every $x_m^n$ and it compensates for amplitude offset 
and scaling effects when distance between time series is computed.

\subsection{Data Smoothing}
\label{sec:dataSmoothing} 

In case the time-series are affected by noise, it might be worthwhile to smooth the time series using classical 
filtering and regression techniques so that the noise is filtered out and the series information is maintained. 
A commonly used de-noising or filtering technique is the kernel-regression technique.
This simple technique starts from the raw data $\bm{\Theta}_n$ which is time dependent (i.e., $\bm{\Theta}_n(t)$) 
and it generates the regressed term $\bm{\tilde{\Theta}}_n$ as follows:

\begin{equation}
  \bm{\tilde{\Theta}}_n = \dfrac{\sum_{t=0}^{T_n} K(t-t') \bm{\Theta}_n}{\sum_{t=0}^{T_n} K(t-t')} 
  \label{eq:smoothing}
\end{equation}
[review this!!!]
where $K(t-t')$ is the kernel used to smooth $\bm{\Theta}_n$.

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.4]{figure_filtered.png}}
    \caption{pppp}
    \label{fig:smoothing}
\end{figure}

\subsection{Data Resampling}
\label{sec:dataResampling} 

Another operation that can be performed in the pre-processing is the re-sampling of $\bm{\Theta}_n$. Recall that 
$\bm{\Theta}_n$ contains the values of the time dependent data variables ${x_1^n,\ldots,x_M^n}$ sampled at specific 
time instants. 
The re-sampling operation aims to reduce those time instants by choosing a new set of time instants (typically a 
smaller set) that preserves the information content of the $\bm{\Theta}_n$.
The motivations behind the choice of this step are the following: less memory intensive and faster computations.
Several re-sampling strategies can be employed:
\begin{itemize}
  \item Uniform: $N$ sample points ($N$ is provided as input) are uniformly located along the time axis
  \item First derivative: $N$ sample points ($N$ is provided as input) concentrated in regions with higher values 
        of the first derivative
  \item Second derivative: $N$ sample points ($N$ is provided as input) concentrated in regions with higher values 
        of the second derivative
  \item samples with derivative greater than fixed value
\end{itemize}
  
\section{Data Representation}
\label{sec:dataRepresentation}   
  
One of the most fundamental modeling choices regarding time dependent data is how each time series is actually 
represented in the data mining process. Reference~\cite{} provides a broad analysis of the
many representation methods. Some of these methods have been implemented in RAVEN; the choice of these implemented methods 
was based on their effectiveness on nuclear engineering applications. In the following sub-sections we will present 
these methods in mode detail along with some preliminary results obtained by RAVEN. 

\subsubsection{Real-valued}
\label{sec:realValued}

The original format of the time series is maintained. This approach does not require any prior knowledge from 
the user so it can be considered a fail-safe approach. On the other side this method
(depending on the data set) can be memory and computationally intensive.

\subsubsection{Polynomial}
\label{sec:polynomial}

The time series is approximated by a Taylor polynomial function (see~\cite{}) up to a 
fixed degree and the vector of coefficients are retained as representatives for the time series. 
Recall that $\bm{\Theta}_n = {x_1^n,\ldots,x_M^n}$ contains the temporal evolution of a set of $M$ 
variables (i.e., $x_1^n = x_1^n (t)$), 
for the Taylor case for example, the approximation is performed as follows for each $x_1^n(t)$:
\begin{equation}
  x_1^n(t) \approx \sum_{\varsigma=0}^C c_\varsigma t^\varsigma
  \label{eq:polynomial}
\end{equation}
The representation process using Taylor expansion replace $x_1^n(t)$ with a vector having dimensionality $C+1$ 
containing the coefficients $c_\varsigma (\varsigma=0,\ldots,C)$.

\subsubsection{Chebyshev}
\label{sec:Chebyshev}

The Chebyshev representation follows exact principle presented above for the Taylor case (see~\cite{}):
\begin{equation}
  x_1^n(t) \approx (\sum_{\varsigma=0}^{C-1} c_\varsigma T_\varsigma) - \frac{1}{2} c_0
  \label{eq:chebyshev}
\end{equation}
where $T_\varsigma(t)$ is the Chebyshev polynomial of order $\varsigma$:
\begin{equation}
\begin{array}{r@{}l}
  T_0(t)&{}=1 \\
  T_1(t)&{}=t \\
  T_2(t)&{}=2 t^2-1) \\
  T_3(t)&{}=4 t^3-3t \\
  T_4(t)&{}=8 t^4-8 t^2+1) \\
        &{}\ldots \\
  T_{\varsigma+1}(t)&{}=2 t T_\varsigma(t)-T_{\varsigma-1}(t)
  \label{eq:ChebyshevPolynomialCoeffs}
\end{array}
\end{equation}
The representation process using Chebyshev expansion replace $x_1^n(t)$ with a vector having dimensionality $C+1$ 
containing all coefficients $c_\varsigma (\varsigma=0,\ldots,C)$. 

\begin{figure}
  \centering
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.3]{cheb1.png}
    \label{fig:sub1}
  \end{subfigure}%
  \begin{subfigure}{.5\textwidth}
    \centering
    \includegraphics[scale=0.3]{cheb1.png}
    \label{fig:sub2}
  \end{subfigure}
  \caption{Chebyshev approximation of a time series for several polynomial degrees.}
  \label{fig:chebyshev}
\end{figure}

\subsubsection{Legendre}
\label{sec:Legendre}

The Legendre polynomials are polynomials (see~\cite{}) of the following form:
\begin{equation}
\begin{array}{r@{}l}
  P_0(t)&{}=1 \\
  P_1(t)&{}=t \\
  P_2(t)&{}=\frac{2 t^2-1)}{2} \\
  P_3(t)&{}=\frac{5 t^3-3t}{2} \\
  P_4(t)&{}=\frac{35 t^4-30 t^2+3}{8} \\
        &{}\ldots \\
  P_{\varsigma+1}(t)&{}=\frac{(2 \varsigma-1)  t P_(\varsigma-1)(t)-(\varsigma-1) P_(\varsigma-2) (t)}{\varsigma}
  \label{eq:LegendrePolynomialCoeffs}
\end{array}
\end{equation}

The representation process using Legendre expansion replace $x_1^n (t)$ with a vector having dimensionality $C+1$ 
containing all coefficients $c_\varsigma  (\varsigma=0,…,C)$. 

\subsubsection{Laguerre}
\label{sec:Laguerre}
The Laguerre polynomials $L_n(t)$ are polynomials (see~\cite{}) of the following form::
\begin{equation}
\begin{array}{r@{}l}
  L_0(t)&{}=1 \\
  L_1(t)&{}=1-t \\
        &{}\ldots \\
  (n+1) L_n(t)&{}-(2n+1-t) L_n(t)+n L_{n-1}(t)=0
  \label{eq:LaguerrePolynomialCoeffs}
\end{array}
\end{equation}

\subsubsection{Hermite}
\label{sec:Hermite}

The Hermite polynomials $H_n(t)$ are polynomials of the following form (see~\cite{}):
\begin{equation}
\begin{array}{r@{}l}
  H_0(t)&{}=1 \\
  H_1(t)&{}=t \\
        &{}\ldots \\
  H_{n+1}(t)&{}-2 t H_n(t)+2 n H_{n-1}(t)=0
  \label{eq:HermitePolynomialCoeffs}
\end{array}
\end{equation}

\subsubsection{Discrete Fourier Transform}
\label{sec:fourier}

Similar to the polynomial representation, the time series is approximated by a Fourier series and the series coefficients 
are retained as representatives for the time series. The Fourier series is as follows 
\begin{equation}
  x_1^n(t) \approx \frac{a_0}{2} (\sum_{\varsigma=1}^{C} (a_\varsigma cos(\varsigma t) + b_\varsigma sin(\varsigma t)) 
  \label{eq:fourier}
\end{equation}

\begin{figure}
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{fourier1.png}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{fourier2.png}
  \end{subfigure}
  \caption{Fourier approximation of a time series for several polynomial degrees.}
  \label{fig:fourier}
\end{figure}

\subsubsection{Singular Value Decomposition (SVD)}
\label{sec:svd}

This method performs an eigenvalues and eigenvectors decomposition of $\bm{\Theta}_n$ and selects a reduced set of eigenvectors. 
Each time series $Η_n$ is represented by the coefficients associated to each eigenvector. 
Note that this decomposition must be performed on all time-series as a whole since SVD decomposition is performed 
on the covariance matrix which is calculated by considering all set of time series and not one time series separately. 

This is performed for each $x_m (m=1,\ldots,M)$ independently by:
\begin{enumerate}
  \item Normalizing the data: zero mean and unit variance (see Section~\ref{})
  \item Resampling the data set so that all time series have been sampled on the exact same time instants
  \item Determining the covariance matrix
  \item Performing SVD of the covariance matrix, i.e., eigenvalues and eigenvectors decomposition~\cite{}. 
        Note that each eigenvector is a time series sampled at the same time instants of the original time series. 
        The eigenvectors can be ranked based on the associated eigenvalues: the space
        reduction can be performed by considering the eigenvector with higher eigenvalues 
  \item Projecting the original time series into the eigenvectors space (either reduced or not) and using the projection 
        coefficients as time series representation format.
\end{enumerate}
 
\begin{figure}
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{SVD1.png}
  \end{subfigure}%
  \begin{subfigure}{.5\linewidth}
    \centering
    \includegraphics[scale=0.3]{SVD3.png}
  \end{subfigure}
  \caption{[]}
  \label{fig:svd}
\end{figure}  

\begin{figure}
    \centering
    \centerline{\includegraphics[scale=0.4]{SVD4.png}} 
    \caption{[]}
    \label{fig:SVDcumulative}
\end{figure} 
  
